# -*- coding: utf-8 -*-
"""571-Final-Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x4_ydWDyNx0SJk328gNV3CeiXZHHJawN
"""

import io
import requests
import json
import pprint as pp

def get_json_objs(url):
    response = requests.get(url)
    file = io.StringIO(response.text)
    return json.load(file)

nextdoor_posts = get_json_objs('https://drive.google.com/uc?export=download&id=10HB1IYT7hLCp9Ui7VoXYawCvSiukkO8C')

pp.pprint(nextdoor_posts[0:5])

"""## How many unique authors are there?"""

unique_authors = set()

for post in nextdoor_posts:
    unique_authors.add(post['author'])

ount_unique_authors = len(unique_authors)

pp.pprint(f"Number of unique authors: {count_unique_authors}")

"""## How many posts does each author have?"""

post_count_by_author = {}

for post in nextdoor_posts:
    author = post['author']

    post_count = post_count_by_author.get(author, 0)

    post_count_by_author[author] = post_count + 1

sorted_authors = sorted(post_count_by_author.items(), key=lambda x: x[1], reverse=True)

for author, count in sorted_authors[0:19]:
    print(f"{author}: {count} posts")

"""## How many unique neighborhoods are there?"""

unique_neighborhoods = set()

for post in nextdoor_posts:
    unique_neighborhoods.add(post['neighborhood'])

count_unique_neighborhoods = len(unique_neighborhoods)

pp.pprint(f"Number of unique neighborhoods: {count_unique_neighborhoods}")

"""## How many posts does each neighborhood have?"""

post_count_by_neighborhood = {}

for post in nextdoor_posts:
    neighborhood = post['neighborhood']

    post_count = post_count_by_neighborhood.get(neighborhood, 0)

    post_count_by_neighborhood[neighborhood] = post_count + 1

sorted_neighborhoods = sorted(post_count_by_neighborhood.items(), key=lambda x: x[1], reverse=True)

for neighborhood, count in sorted_neighborhoods[0:19]:
    print(f"{neighborhood}: {count} posts")

"""## How many unique authors are there from each neighborhood?"""

unique_authors_by_neighborhood = {}

for post in nextdoor_posts:
    author = post['author']
    neighborhood = post['neighborhood']

    unique_authors = unique_authors_by_neighborhood.setdefault(neighborhood, set())

    unique_authors.add(author)

sorted_neighborhood_authors = sorted(unique_authors_by_neighborhood.items(), key=lambda x: len(x[1]), reverse=True)

for neighborhood, unique_authors_set in sorted_neighborhood_authors[0:19]:
    print(f"{neighborhood}: {len(unique_authors_set)} unique authors")

"""## Topic Modeling: Which Words / Topics Appear Most Frequently?"""

import string
punct = string.punctuation
table = str.maketrans(punct, ' ' * len(punct))

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')

def clean_text(text):

  text = text.lower()
  text = text.translate(table)

  words = text.split()
  words_no_sw = list()

  for word in words:
    if word not in sw:
      words_no_sw.append(word)

  clean_text = ' '.join(words_no_sw)

  return clean_text

clean_posts = []

for i in range(0, len(nextdoor_posts)):

  cleaned_post = clean_text(nextdoor_posts[i]['comment'])
  clean_posts.append(cleaned_post)

pp.pprint(clean_posts[0:9])

!pip install bertopic
from bertopic import BERTopic

topic_model = BERTopic()
topics, probs = topic_model.fit_transform(clean_posts)

topic_model.visualize_barchart()

"""## How many times do certain key words appear in the posts?"""

count_searing = 0

for post in clean_posts:

    count_searing += post.count("searing")

print(f"The word 'searing' appears {count_searing} times in the data set.")

unique_posts_with_searing = set()

for post in clean_posts:

    if "searing" in post:
        unique_posts_with_searing.add(post)

print(f"The word 'searing' is mentioned in {len(unique_posts_with_searing)} unique posts.")

count_anderson = 0

for post in clean_posts:

    count_anderson += post.count("anderson")

print(f"The word 'anderson' appears {count_anderson} times in the data set.")

unique_posts_with_anderson = set()

for post in clean_posts:

    if "anderson" in post:
        unique_posts_with_anderson.add(post)

print(f"The word 'anderson' is mentioned in {len(unique_posts_with_anderson)} unique posts.")

count_election_vote = 0

for post in clean_posts:

    count_election_vote += post.count("election") + post.count("vote")

print(f"The words 'election' or 'vote' appear {count_election_vote} times in the data set.")

unique_posts_with_election_vote = set()

for post in clean_posts:

    if "election" in post or "vote" in post:
        unique_posts_with_election_vote.add(post)

print(f"The words 'election' or 'vote' are mentioned in {len(unique_posts_with_election_vote)} unique posts.")

count_affordable_housing = 0

for post in clean_posts:

    count_affordable_housing += post.count("affordable housing")

print(f"The term 'affordable housing' appears {count_affordable_housing} times in the data set.")

unique_posts_with_affordable_housing = set()

for post in clean_posts:

    if "affordable housing" in post:
        unique_posts_with_affordable_housing.add(post)

print(f"The term 'affordable housing' is mentioned in {len(unique_posts_with_affordable_housing)} unique posts.")

count_rezon = 0

for post in clean_posts:

    count_rezon += post.count("rezon")

print(f"The terms 'rezoning', 'rezone', and/or 'rezoned' appear {count_rezon} times in the data set.")

unique_posts_rezon = set()

for post in clean_posts:

    if "rezon" in post:
        unique_posts_rezon.add(post)

print(f"The terms 'rezoning', 'rezone', and/or 'rezoned' are mentioned in {len(unique_posts_rezon)} unique posts.")

"""## Alright, let's do some sentiment analysis on posts with these terms"""

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')
sid = SentimentIntensityAnalyzer()

searing_posts = list()

for i in range(0, len(nextdoor_posts)):
  comment = clean_text(nextdoor_posts[i]['comment'])

  if "searing" in comment:
      searing_posts.append(nextdoor_posts[i])

for post in searing_posts:
    post['sentiment_score'] = sid.polarity_scores(post['comment'])['compound']

pp.pprint(searing_posts[0:2])

neighborhood_sentiments = {}

for post in searing_posts:
    neighborhood = post['neighborhood']
    sentiment_score = post['sentiment_score']

    if neighborhood not in neighborhood_sentiments:
        neighborhood_sentiments[neighborhood] = {'scores': [], 'count': 0}

    neighborhood_sentiments[neighborhood]['scores'].append(sentiment_score)
    neighborhood_sentiments[neighborhood]['count'] += 1

mean_sentiments = {
    neighborhood: sum(data['scores']) / data['count'] for neighborhood, data in neighborhood_sentiments.items()
}

sorted_means = sorted(mean_sentiments.items(), key=lambda x: x[1], reverse=True)

for neighborhood, mean_sentiment in sorted_means:
    print(f"Neighborhood: {neighborhood}, Mean Sentiment Score: {mean_sentiment:.3f}")

def get_unique_authors_by_neighborhood(dict_list):
    unique_authors_by_neighborhood = {}

    for post in dict_list:
        author = post.get('author')
        neighborhood = post.get('neighborhood')

        if author is not None and neighborhood is not None:
            unique_authors = unique_authors_by_neighborhood.setdefault(neighborhood, set())

            unique_authors.add(author)

    neighborhood_author_counts = {neighborhood: len(authors) for neighborhood, authors in unique_authors_by_neighborhood.items()}

    return neighborhood_author_counts

searing_authors = get_unique_authors_by_neighborhood(searing_posts)
pp.pprint(searing_authors)

pp.pprint(mean_sentiments)

neighborhood_counts = {neighborhood: data['count'] for neighborhood, data in neighborhood_sentiments.items()}
pp.pprint(neighborhood_counts)

neighborhood_coords ={'15-501/Mt Moriah': {'lat': 35.952918, 'lon': -78.994416},
 'Barred Owl Creek': {'lat': 35.923128, 'lon': -79.093987},
 'Booker Creek': {'lat': 35.9452, 'lon': -79.023},
 'Cates Farm': {'lat': 35.932651, 'lon': -79.092796},
 'Chapel View': {'lat': 35.949795, 'lon': -79.059816},
 'Colony Lake': {'lat': 35.938647, 'lon': -79.007733},
 'Estes Hills': {'lat': 35.933458, 'lon': -79.044309},
 'Greenwood': {'lat': 35.915301, 'lon': -79.030190},
 'Hillsborough Road': {'lat': 35.918484, 'lon': -79.080912},
 'Lake Forest': {'lat': 35.946453, 'lon': -79.036027},
 'MLK Park': {'lat': 35.926391, 'lon': -79.088516},
 'Morgan Creek': {'lat': 35.888471, 'lon': -79.052881},
 'Northwood Drive': {'lat': 35.967532, 'lon': -79.064149},
 'Oxford Hills': {'lat': 35.940102, 'lon': -79.028291},
 'Southern Village': {'lat': 35.884335, 'lon': -79.065325},
 'The Highlands': {'lat': 35.952292, 'lon': -79.083643},
 'Umstead': {'lat': 35.921471, 'lon': -79.063239},
 'Vineyard Square': {'lat': 35.955134, 'lon': -79.067594},
 'Westhampton/Calvander': {'lat': 35.929506, 'lon': -79.118478}}

neighborhood_lat = {neighborhood: data['lat'] for neighborhood, data in neighborhood_coords.items()}
neighborhood_lon = {neighborhood: data['lon'] for neighborhood, data in neighborhood_coords.items()}

searing_neighborhood_dict = [{'neighborhood': neighborhood, 'posts': neighborhood_counts[neighborhood], 'authors': searing_authors[neighborhood], 'mean_sentiment': mean_sentiments[neighborhood], 'lat': neighborhood_lat[neighborhood], 'lon': neighborhood_lon[neighborhood]} for neighborhood in neighborhood_counts]

pp.pprint(searing_neighborhood_dict[0])

import pandas as pd
import plotly.express as px

px.set_mapbox_access_token('pk.eyJ1IjoibHVjYXN0aG9tYWUiLCJhIjoiY2xweDV3OHh0MDZ3eTJycDdtNXJid2hndyJ9.pQMGL6ZXaRUCHSbXUVoO0w')

# Data with latitude/longitude and values
fig = px.scatter_mapbox(searing_neighborhood_dict, lat = 'lat', lon = 'lon', size = 'posts', size_max=120, color = 'mean_sentiment', hover_data=('neighborhood', 'posts', 'authors', 'mean_sentiment'), text='neighborhood',
                        color_continuous_scale='RdYlBu', range_color=(-1, 1), opacity=0.7, zoom = 11.5, mapbox_style = 'light', height=800, width=1200, center={'lat':35.9343333149562, 'lon':-79.05655231022988},
                        title='Positive/Negative Sentiment Scores for Nextdoor Posts About Adam Searing')

fig.show()